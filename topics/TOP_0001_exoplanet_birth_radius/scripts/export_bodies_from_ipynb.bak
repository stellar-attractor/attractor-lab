from __future__ import annotations

import json
import re
from pathlib import Path
from typing import Iterable

# =============================================================================
# CONFIG
# =============================================================================

TOPIC_DIR = Path(__file__).resolve().parents[1]
NB_DIR = TOPIC_DIR / "notebooks"
OUT_DIR = TOPIC_DIR / "tex" / "_tmp"
OUT_DIR.mkdir(parents=True, exist_ok=True)

# =============================================================================
# Unicode ‚Üí LaTeX sanitization (for pdfLaTeX)
# =============================================================================

UNICODE_TO_LATEX = {
    # math / relations
    "‚â†": r"$\neq$",
    "‚â§": r"$\leq$",
    "‚â•": r"$\geq$",
    "‚âà": r"$\approx$",
    "‚âÉ": r"$\simeq$",
    "¬±": r"$\pm$",
    "√ó": r"$\times$",
    "¬∑": r"$\cdot$",
    "‚Üí": r"$\rightarrow$",
    "‚Üê": r"$\leftarrow$",
    "‚Üî": r"$\leftrightarrow$",

    # typography
    "‚Äî": r"---",
    "‚Äì": r"--",
    "‚Ä¶": r"\ldots{}",
    "‚Äú": r"``",
    "‚Äù": r"''",
    "‚Äû": r"``",
    "‚Äô": r"'",
    "‚Äò": r"`",

    # rare subscripts
    "‚Çç": r"$_{(}$",
    "‚Çé": r"$_{)}$",

    # greek
    "Œ±": r"$\alpha$",
    "Œ≤": r"$\beta$",
    "Œ≥": r"$\gamma$",
    "Œ¥": r"$\delta$",
    "Œµ": r"$\epsilon$",
    "Œª": r"$\lambda$",
    "Œº": r"$\mu$",
    "œÄ": r"$\pi$",
    "œÉ": r"$\sigma$",
    "Œ©": r"$\Omega$",

    # emoji
    "üìå": r"\textbf{[NOTE]} ",
    "‚úÖ": r"\textbf{[OK]} ",
    "‚ö†Ô∏è": r"\textbf{[WARN]} ",
    "‚ùó": r"\textbf{[!]} ",
    "üîç": r"\textbf{[CHECK]} ",
    "üí°": r"\textbf{[IDEA]} ",
    "üëâ": r"$begin:math:text$\\rightarrow$end:math:text$ ",
    "üåå": r"\textbf{[SPACE]} ",
    "‚òâ": r"$begin:math:text$\\odot$end:math:text$",
}

LATEX_SPECIALS = {
    "#": r"\#",
    "$": r"\$",
    "%": r"\%",
    "&": r"\&",
    "_": r"\_",
    # keep braces unescaped (you already decided this)
    # "{": r"\{",
    # "}": r"\}",
    "~": r"\textasciitilde{}",
    "^": r"\textasciicircum{}",
}

# Detect math blocks and keep them intact (do not escape inside).
# Supports: $...$, $$...$$, $begin:math:text$\.\.\.$end:math:text$, $begin:math:display$\.\.\.$end:math:display$
_MATH_RE = re.compile(
    r"(\$\$.*?\$\$|\$.*?\$|\\$begin:math:text$\.\+\?\\\\$end:math:text$|\\$begin:math:display$\.\+\?\\\\$end:math:display$)",
    re.DOTALL,
)

def sanitize_tex(text: str) -> str:
    """Make text safe for pdfLaTeX without destroying math."""
    parts: list[str] = []
    last = 0
    for m in _MATH_RE.finditer(text):
        parts.append(_sanitize_non_math(text[last:m.start()]))
        parts.append(m.group(0))  # keep math as-is
        last = m.end()
    parts.append(_sanitize_non_math(text[last:]))
    return "".join(parts)

def _sanitize_non_math(t: str) -> str:
    """
    Two-phase:
      1) Replace Unicode tokens with placeholders
      2) Escape LaTeX specials in the remaining text
      3) Restore placeholders as raw LaTeX (do NOT escape them)
    """
    if not t:
        return t

    placeholders: dict[str, str] = {}
    for i, (u, latex) in enumerate(UNICODE_TO_LATEX.items()):
        ph = f"@@U2L{i}@@"
        if u in t:
            t = t.replace(u, ph)
            placeholders[ph] = latex

    out_chars: list[str] = []
    for ch in t:
        out_chars.append(LATEX_SPECIALS.get(ch, ch))
    t = "".join(out_chars)

    for ph, latex in placeholders.items():
        t = t.replace(ph, latex)

    return t

# =============================================================================
# Sanitize math
# =============================================================================

_MATH_INLINE_OR_DISPLAY_RE = re.compile(r"(\$\$.*?\$\$|\$.*?\$)", re.DOTALL)

_SUBSCRIPT_WORD_RE = re.compile(r"_(?!\{)([A-Za-z]{2,})\b")  # _ISM, _birth

def _fix_math(s: str) -> str:
    def repl(m: re.Match) -> str:
        word = m.group(1)
        return r"_{\mathrm{" + word + "}}"

    s = _SUBSCRIPT_WORD_RE.sub(repl, s)

    # –µ–¥–∏–Ω–∏—Ü—ã –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –≤–Ω—É—Ç—Ä–∏ math
    s = s.replace("dex", r"\mathrm{dex}")
    s = s.replace("kpc", r"\mathrm{kpc}")

    return s

def _sanitize_preserving_math(s: str) -> str:
    parts = []
    last = 0
    for m in _MATH_INLINE_OR_DISPLAY_RE.finditer(s):
        parts.append(sanitize_tex(s[last:m.start()]))   # –≤–Ω–µ math ‚Äî —Å–∞–Ω–∏—Ç–∞–π–∑–∏–º
        parts.append(_fix_math(m.group(0)))             # –≤–Ω—É—Ç—Ä–∏ math ‚Äî —á–∏–Ω–∏–º –∏–Ω–¥–µ–∫—Å—ã
        last = m.end()
    parts.append(sanitize_tex(s[last:]))
    return "".join(parts)

# =============================================================================
# Markdown ‚Üí LaTeX (minimal, controlled)
# =============================================================================

# Markdown image: ![alt](path)
_IMG_MD_RE = re.compile(r"^\s*!\[(?P<alt>[^\]]*)\]\((?P<path>[^)]+)\)\s*$")

# Caption line after image:
#   *Figure 1. Data Samples*
#   *Fig. 2. Something ...*
_FIGCAP_MD_RE = re.compile(
    r"^\s*\*(?:Figure|Fig\.?)\s*\d+\s*\.?\s*(?P<cap>.*?)\s*\*\s*$",
    re.IGNORECASE,
)

_ENUM_MD_RE = re.compile(r"^\s*(\d+)\.\s+(.*)$")
_HR_MD_RE   = re.compile(r"^\s*---+\s*$")

# "- text" or "* text" bullet (markdown)
_BULLET_MD_RE = re.compile(r"^\s*[-*]\s+(?P<txt>.+?)\s*$")

def md_to_tex(md: str) -> str:
    lines = md.splitlines()
    out: list[str] = []

    in_enum = False
    in_itemize = False

    i = 0
    while i < len(lines):
        ln = lines[i].rstrip()

        # --- markdown image: ![alt](path) ---
        m = _IMG_MD_RE.match(ln)
        if m:
            # close lists before a figure
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False
            if in_enum:
                out.append(r"\end{enumerate}")
                in_enum = False

            alt = (m.group("alt") or "").strip()
            pth = (m.group("path") or "").strip()

            # Lookahead: skip blank lines, then check "*Figure N. ...*"
            j = i + 1
            while j < len(lines) and lines[j].strip() == "":
                j += 1

            figcap = ""
            if j < len(lines):
                mcap = _FIGCAP_MD_RE.match(lines[j].rstrip())
                if mcap:
                    figcap = (mcap.group("cap") or "").strip()

            caption = figcap or alt

            out.append(r"\begin{figure}[!ht]")
            out.append(r"\centering")
            out.append(r"\includegraphics[width=\linewidth]{" + pth + "}")
            if caption:
                out.append(r"\caption{" + sanitize_tex(caption) + "}")
            out.append(r"\end{figure}")

            # Consume caption line if present (plus any blank lines between)
            if figcap:
                i = j + 1
            else:
                i += 1
            continue

        # --- headers (close lists before headers) ---
        if ln.startswith("##### "):
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False
            if in_enum:
                out.append(r"\end{enumerate}")
                in_enum = False
            out.append(r"\subsubsection{" + sanitize_tex(ln[6:]) + "}")
            i += 1
            continue

        if ln.startswith("#### "):
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False
            if in_enum:
                out.append(r"\end{enumerate}")
                in_enum = False
            out.append(r"\subsubsection*{" + sanitize_tex(ln[5:]) + "}")
            i += 1
            continue

        if ln.startswith("### "):
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False
            if in_enum:
                out.append(r"\end{enumerate}")
                in_enum = False
            out.append(r"\subsubsection*{" + sanitize_tex(ln[4:]) + "}")
            i += 1
            continue

        if ln.startswith("## "):
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False
            if in_enum:
                out.append(r"\end{enumerate}")
                in_enum = False
            out.append(r"\subsection*{" + sanitize_tex(ln[3:]) + "}")
            i += 1
            continue

        if ln.startswith("# "):
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False
            if in_enum:
                out.append(r"\end{enumerate}")
                in_enum = False
            out.append(r"\section*{" + sanitize_tex(ln[2:]) + "}")
            i += 1
            continue

        # --- enumerated list item: "1. text" ---
        m_enum = _ENUM_MD_RE.match(ln)
        if m_enum:
            # close itemize if it was open
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False

            if not in_enum:
                out.append(r"\begin{enumerate}")
                in_enum = True

            txt = m_enum.group(2).strip()
            out.append(r"\item " + _sanitize_preserving_math(txt))
            i += 1
            continue

        # --- bullet list item: "- text" / "* text" ---
        m_b = _BULLET_MD_RE.match(ln)
        if m_b:
            # close enumerate if we switch to bullets
            if in_enum:
                out.append(r"\end{enumerate}")
                in_enum = False

            if not in_itemize:
                out.append(r"\begin{itemize}")
                in_itemize = True

            txt = (m_b.group("txt") or "").strip()
            out.append(r"\item " + _sanitize_preserving_math(txt))
            i += 1
            continue
        else:
            # if we were in itemize and current line is not a bullet, close itemize
            if in_itemize:
                out.append(r"\end{itemize}")
                in_itemize = False

        # --- normal text line (stay inside enumerate if it's open) ---
        ln2 = re.sub(r"\*\*(.+?)\*\*", r"\\textbf{\1}", ln)
        ln2 = re.sub(r"\*(.+?)\*", r"\\emph{\1}", ln2)
        out.append(_sanitize_preserving_math(ln2))
        i += 1

    # close any open lists at EOF
    if in_itemize:
        out.append(r"\end{itemize}")
    if in_enum:
        out.append(r"\end{enumerate}")

    return "\n".join(out)

# =============================================================================
# Notebook processing
# =============================================================================

def iter_notebooks() -> Iterable[Path]:
    for nb in sorted(NB_DIR.glob("*.ipynb")):
        if nb.name.startswith("."):
            continue
        yield nb

def extract_markdown_cells(nb_path: Path) -> str:
    data = json.loads(nb_path.read_text(encoding="utf-8"))
    chunks: list[str] = []

    for cell in data.get("cells", []):
        if cell.get("cell_type") == "markdown":
            src = "".join(cell.get("source", []))
            chunks.append(md_to_tex(src))

    return "\n\n".join(chunks).strip() + "\n"

# =============================================================================
# Main
# =============================================================================

def main() -> None:
    exported = 0

    for nb in iter_notebooks():
        stem = nb.stem
        body_tex = extract_markdown_cells(nb)

        if not body_tex.strip():
            print(f"SKIP (no markdown): {nb.name}")
            continue

        out_path = OUT_DIR / f"{stem}_body.tex"
        out_path.write_text(body_tex, encoding="utf-8")
        print(f"OK: {out_path.name}")
        exported += 1

    print(f"\nExported bodies: {exported}")
    print("Location:", OUT_DIR)

if __name__ == "__main__":
    main()